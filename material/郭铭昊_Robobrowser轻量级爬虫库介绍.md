{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robobrowser——一个轻量级爬虫库\n",
    "---------------------------------------\n",
    ">常用于爬虫和简单的web测试，纯python编写，用起来很方便，语法自然，且易学。另外，robobrowser建立在requests和BeautifulSoup之上，容易被人们接受。官方介绍Robobrowser是：Your friendly neighborhood web scraper。\n",
    "\n",
    "## 实现原理\n",
    "---------------------------------------\n",
    "### 要点\n",
    "- RoboState类里，页面上内容的抓取和处理实际上委托给了BeautifulSoup。RoboState类的_parsed对象实际上就是BeautifulSoup的实例；\n",
    "\n",
    "- RoboState类中保存了每个请求的响应内容——response.content；\n",
    "\n",
    "- RoboBrowser类里，发送请求的方法实际上委托给了requests类——session;\n",
    "\n",
    "- RoboBrowser类里比较复杂就是保存每次访问的状态，以及实现back和forward功能。其主要思想是把所有的访问历史都放在内存里，然后通过游标去访问；\n",
    "\n",
    "- 每次页面发生变化，也就是open和submit_form之后都会调用_update_state方法去更新当前状态；\n",
    "- [点击这里](https://github.com/jmcarp/robobrowser) 了解详情.\n",
    "\n",
    "### 流程梳理\n",
    "- RoboBrowser()实例化的时候，会new 1个requests的session用于发送http请求,同时初始化游标为－1并且当前的status列表初始化为空;\n",
    "\n",
    "- RoboBrowser.open(url)方法调用时，session对象会访问具体的url，然后更新游标和status列表。基本思想是往status列表里append 1个新new出来的RoboState对象；\n",
    "\n",
    "- RoboBrowser.find()方法调用时，使用当前游标处的state对象的_parsed对象的find方法去抓取页面内容，实际上就是BeautifulSoup的find方法；\n",
    "\n",
    "## 安装Robobrowser\n",
    "---------------------------------------\n",
    "- 使用pip安装：<pre><code>      pip install robobrowser -i http://pypi.douban.com/simple/\n",
    "</code></pre>\n",
    "\n",
    "## 小例子\n",
    "---------------------------------------\n",
    "- **新建1个start.py文本文件，代码如下**\n",
    "<pre><code>      import re\n",
    "        from robobrowser import RoboBrowser\n",
    "\n",
    "        b = RoboBrowser(history=True)\n",
    "        b.open('http://itest.info/courses/2')\n",
    "\n",
    "        title = b.select('.headline h2')\n",
    "        print title[0].text\n",
    "\n",
    "        infos = b.select('h4')\n",
    "\n",
    "        for info in infos:\n",
    "            print info.text\n",
    "</code></pre>\n",
    "\n",
    "## 文档查询\n",
    "---------------------------------------\n",
    "- **robobrowser自带文档shu说明，命令行运行：**\n",
    "<pre><code>        python -m pydoc -p 1234\n",
    "</code></pre>\n",
    "\n",
    "## 简单的爬虫\n",
    "---------------------------------------\n",
    "### 代码讲解\n",
    "<pre><code>        import re\n",
    "        from robobrowser import RoboBrowser\n",
    "\n",
    "        url = 'http://itest.info/courses/2'\n",
    "        b = RoboBrowser(history=True)\n",
    "        b.open(url)\n",
    "\n",
    "        class_name = b.select('.headline h2')\n",
    "        print class_name[0].text\n",
    "\n",
    "        class_desc = b.select('.tag-box')\n",
    "        print class_desc[0].text \n",
    "\n",
    "        class_time = b.select('h4')\n",
    "        print class_time[0].text\n",
    "\n",
    "        teacher = b.select('.thumbnail-style h3')\n",
    "        print teacher[0].text\n",
    "\n",
    "        qq = b.find(text=re.compile('QQ'))\n",
    "        print qq\n",
    "\n",
    "        qq_group = b.find(text=re.compile('\\+selenium'))\n",
    "        print qq_group\n",
    "</code></pre>\n",
    "  \n",
    "- b = RoboBrowser(history=True) b.open(url) 用来创建browser和打开url；\n",
    "- b.select() 方法可以接受css选择器，返回页面上所有符合条件的元素的集合，也就是说返回的是list，可以进行迭代；\n",
    "- b.find()，只返回1个精确的结果；\n",
    "- 注意，find和select方法返回的均是Beautiful Soup的tag对象或对象集合；\n",
    "\n",
    "## 抓取制定内容\n",
    "---------------------------------------\n",
    "### 背景\n",
    " robobrowser支持Beautiful Soup，一般来说通过下面3个方法获取页面上感兴趣的内容\n",
    "     - find\n",
    "     - find_all\n",
    "     - select\n",
    "     \n",
    "### find方法\n",
    "  find方法是返回页面上符合条件的第1个元素。<pre><code>        import re\n",
    "        from robobrowser import RoboBrowser\n",
    "        url = 'http://itest.info/courses/2'\n",
    "        b = RoboBrowser(history=True)\n",
    "        b.open(url)\n",
    "\n",
    "        title = b.find('title')  \n",
    "        print title.text\n",
    "\n",
    "        img = b.find(id='logo-header')\n",
    "        print img['src']\n",
    "\n",
    "        print b.find(href='/courses/4').text\n",
    "\n",
    "        print b.find(class_='active', text=re.compile('python')).text\n",
    "</code></pre>\n",
    "\n",
    "### find_all方法\n",
    " find_all方法的用法跟find基本相同，但是find_all会返回所有符合条件的tag的集合(ResultSet)。<pre><code>        import re\n",
    "        from robobrowser import RoboBrowser\n",
    "\n",
    "        url = 'http://itest.info/courses/2'\n",
    "        b = RoboBrowser(history=True)\n",
    "        b.open(url)\n",
    "\n",
    "        all_links = b.find_all('a')  \n",
    "        for link in all_links:\n",
    "        print link.text\n",
    "\n",
    "        divs = b.find_all(class_='container')\n",
    "        print divs\n",
    "\n",
    "        first_two_p = b.find_all('p', limit=2)\n",
    "        print first_two_p\n",
    "\n",
    "        print b.find_all(['meta', 'img'])\n",
    "</code></pre>\n",
    "\n",
    "### select方法\n",
    " select方法支持css选择器，返回的是list。<pre><code>        import re\n",
    "        from robobrowser import RoboBrowser\n",
    "\n",
    "        url = 'http://itest.info/courses/2'\n",
    "        b = RoboBrowser(history=True)\n",
    "        b.open(url)\n",
    "\n",
    "        all_links = b.select('a')  \n",
    "        for link in all_links:\n",
    "        print link.text\n",
    "\n",
    "        divs = b.select('.container')\n",
    "        print len(divs)\n",
    "</code></pre>\n",
    "\n",
    "### 其他技巧\n",
    "- 找到页面上所有具有id属性的元素  b.find_all(id=True)\n",
    "- 不递归查找元素。也就是说只在的直接子后代中查找  b.find('p', recursive=False)\n",
    "\n",
    "##  follow_link\n",
    "---------------------------------------\n",
    "### 方法介绍\n",
    " robobrowser的  follow_link  方法可以点击链接并自动完成跳转。<pre><code>        import re\n",
    "        from robobrowser import RoboBrowser\n",
    "        url = 'http://www.qq.com/'\n",
    "        b = RoboBrowser(history=True)\n",
    "        b.open(url)\n",
    "\n",
    "        today_top = b.find(id='todaytop').a  \n",
    "        print today_top['href']\n",
    "        b.follow_link(today_top)\n",
    "\n",
    "        title = b.select('.hd h1')[0]\n",
    "        print '*************************************'\n",
    "        print title.text\n",
    "        print '*************************************'\n",
    "\n",
    "        print b.find(id='articleContent').text\n",
    "</code></pre>\n",
    "- follow_link的用法，一般来说都是用find/select/find_all方法过滤出相应的链接，然后调用b.follow_link(link)的方式去点击该链接。\n",
    "\n",
    "## 表单操作\n",
    "---------------------------------------\n",
    "### 方法介绍\n",
    "<pre><code>        import re\n",
    "        from robobrowser import RoboBrowser\n",
    "\n",
    "        url = 'http://testerhome.com/account/sign_in/'\n",
    "        b = RoboBrowser(history=True)\n",
    "        b.open(url)\n",
    "\n",
    "        login_form = b.get_form(action='/account/sign_in')\n",
    "        print login_form\n",
    "\n",
    "        login_form['user[login]'].value = 'your account'\n",
    "        login_form['user[password]'].value = 'your password'\n",
    "\n",
    "        b.submit_form(login_form)\n",
    "        print b.select('.alert.alert-success')[0].text\n",
    "</code></pre>\n",
    "\n",
    "- get_form  方法用来抓取form;\n",
    "- submit_form  方法用来提交表单;\n",
    "- form[name].value=  方法用来给文本框赋值，也就是说往文本框里写内容;\n",
    "\n",
    "## Beauiful Soup的过滤器\n",
    "---------------------------------------\n",
    "### 字符串\n",
    "最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的b标签:<pre><code>        soup.find_all('b')\n",
    "</code></pre>\n",
    "\n",
    "### 正则表达式\n",
    "如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示  body  和  b  标签都应该被找到:<pre><code>        import re\n",
    "        for tag in soup.find_all(re.compile(\"^b\")):\n",
    "            print(tag.name)\n",
    "</code></pre>\n",
    "\n",
    "下面代码找出所有名字中包含”t”的标签:<pre><code>        for tag in soup.find_all(re.compile(\"t\")):\n",
    "            print(tag.name)\n",
    "</code></pre>\n",
    "\n",
    "### 列表\n",
    "如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有  a  标签和  b  标签:<pre><code>        soup.find_all([\"a\", \"b\"]) \n",
    "</code></pre>\n",
    "\n",
    "### True\n",
    "True 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点<pre><code>        for tag in soup.find_all(True):\n",
    "            print(tag.name)\n",
    "</code></pre>\n",
    "\n",
    "### 方法\n",
    "如果没有合适过滤器,那么还可以定义一个方法,方法只接受一个元素参数 ,如果这个方法返回 True 表示当前元素匹配并且被找到,如果不是则反回 False\n",
    "\n",
    "下面方法校验了当前元素,如果包含 class 属性却不包含 id 属性,那么将返回 True:<pre><code>        def has_class_but_no_id(tag):\n",
    "            return tag.has_attr('class') and not tag.has_attr('id')\n",
    "</code></pre>\n",
    "\n",
    "将这个方法作为参数传入 find_all() 方法,将得到所有标签：<pre><code>        soup.find_all(has_class_but_no_id)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 常用于爬虫和简单的web测试，纯python编写，用起来很方便。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
